"""
hermes_simple.py - ExÃ©cuteur SQL pour HermÃ¨s

ReÃ§oit du SQL de l'Agent, valide, exÃ©cute, retourne les rÃ©sultats.
Remplace l'ancien HermÃ¨s complexe (parsing, scoring, weights).

v0.8.5 - Ajout de get_segments() pour consultation avant suppression
v0.10.5 â€” 18 outils disponibles (ajout explore_links)

Usage:
    from actions.hermes_simple import execute_sql
    result = execute_sql("SELECT timestamp, resume_texte FROM metadata WHERE ...")
"""

import sqlite3
import re
from pathlib import Path
from typing import Dict, List, Any

# === CONFIGURATION ===
DB_PATH = Path("~/Dropbox/aiterego_memory/metadata.db").expanduser()
IRIS_KNOWLEDGE_DB = Path("~/Dropbox/aiterego_memory/iris/iris_knowledge.db").expanduser()

# === VALIDATION ===
ALLOWED_TABLES = {"metadata"}
FORBIDDEN_KEYWORDS = {"INSERT", "UPDATE", "DELETE", "DROP", "CREATE", "ALTER", "TRUNCATE"}


def validate_sql(sql: str) -> tuple[bool, str]:
    """
    Valide que le SQL est sÃ©curitaire.
    
    Returns:
        (is_valid, error_message)
    """
    sql_upper = sql.upper().strip()
    
    # Doit commencer par SELECT
    if not sql_upper.startswith("SELECT"):
        return False, "Seules les requÃªtes SELECT sont autorisÃ©es"
    
    # Pas de mots-clÃ©s dangereux
    for keyword in FORBIDDEN_KEYWORDS:
        if keyword in sql_upper:
            return False, f"Mot-clÃ© interdit: {keyword}"
    
    # Doit contenir "FROM metadata"
    if "FROM METADATA" not in sql_upper:
        return False, "Seule la table 'metadata' est autorisÃ©e"
    
    return True, ""


def execute_sql(sql: str) -> Dict[str, Any]:
    """
    ExÃ©cute une requÃªte SQL sur la base metadata.
    
    Args:
        sql: RequÃªte SQL (SELECT uniquement)
        
    Returns:
        dict avec:
            - status: "success" ou "error"
            - results: liste de dictionnaires (lignes)
            - count: nombre de rÃ©sultats
            - error: message d'erreur si Ã©chec
    """
    # 1. Valider
    is_valid, error = validate_sql(sql)
    if not is_valid:
        return {
            "status": "error",
            "error": error,
            "sql": sql
        }
    
    # 2. ExÃ©cuter
    try:
        conn = sqlite3.connect(str(DB_PATH))
        conn.row_factory = sqlite3.Row
        
        cursor = conn.execute(sql)
        rows = cursor.fetchall()
        
        # Convertir en liste de dicts
        results = [dict(row) for row in rows]
        
        conn.close()
        
        return {
            "status": "success",
            "results": results,
            "count": len(results),
            "sql": sql
        }
        
    except sqlite3.Error as e:
        return {
            "status": "error",
            "error": f"Erreur SQLite: {str(e)}",
            "sql": sql
        }
    except Exception as e:
        return {
            "status": "error",
            "error": f"Erreur inattendue: {str(e)}",
            "sql": sql
        }


def format_results_for_agent(results: List[Dict]) -> str:
    """
    Formate les rÃ©sultats SQL pour injection dans le prompt de l'Agent.
    """
    if not results:
        return "Aucun rÃ©sultat trouvÃ© dans la mÃ©moire."
    
    lines = [f"--- {len(results)} RÃ‰SULTAT(S) TROUVÃ‰(S) ---\n"]
    
    for i, row in enumerate(results, 1):
        lines.append(f"[{i}]")
        for key, value in row.items():
            if value is not None:
                # Tronquer les valeurs longues
                str_value = str(value)
                if len(str_value) > 200:
                    str_value = str_value[:200] + "..."
                lines.append(f"  {key}: {str_value}")
        lines.append("")
    
    return "\n".join(lines)

# === OPÃ‰RATIONS PILIERS ===

def validate_pilier_sql(sql: str) -> tuple[bool, str]:
    """
    Valide que le SQL est une opÃ©ration pilier autorisÃ©e.
    
    OpÃ©rations permises:
    - UPDATE metadata SET pilier = ... WHERE id = ...
    - INSERT INTO piliers (...)
    - UPDATE piliers SET ... WHERE id = ...
    - DELETE FROM piliers WHERE id = ...
    
    Returns:
        (is_valid, error_message)
    """
    sql_upper = sql.upper().strip()
    sql_clean = ' '.join(sql_upper.split())  # Normaliser les espaces
    
    # 1. UPDATE metadata SET pilier = ... (seule modif autorisÃ©e sur metadata)
    if sql_upper.startswith("UPDATE METADATA"):
        # VÃ©rifier que seul le champ 'pilier' est modifiÃ©
        if "SET PILIER" in sql_clean or "SET PILIER" in sql_upper:
            # Interdire la modification d'autres champs
            # Pattern: UPDATE METADATA SET PILIER = X WHERE ...
            set_clause = sql_upper.split("SET")[1].split("WHERE")[0] if "WHERE" in sql_upper else sql_upper.split("SET")[1]
            # Ne doit contenir que "pilier"
            fields_modified = [f.strip().split("=")[0].strip() for f in set_clause.split(",")]
            if all(f == "PILIER" for f in fields_modified):
                if "WHERE" in sql_upper and "ID" in sql_upper:
                    return True, ""
                return False, "UPDATE metadata SET pilier doit inclure WHERE id = ..."
        return False, "Seul le champ 'pilier' peut Ãªtre modifiÃ© dans metadata"
    
    # 2. INSERT INTO piliers (...)
    if sql_upper.startswith("INSERT INTO PILIERS"):
        return True, ""
    
    # 3. UPDATE piliers SET ... WHERE id = ...
    if sql_upper.startswith("UPDATE PILIERS"):
        if "WHERE" in sql_upper and "ID" in sql_upper:
            return True, ""
        return False, "UPDATE piliers doit inclure WHERE id = ..."
    
    # 4. DELETE FROM piliers WHERE id = ...
    if sql_upper.startswith("DELETE FROM PILIERS"):
        if "WHERE" in sql_upper and "ID" in sql_upper:
            return True, ""
        return False, "DELETE FROM piliers doit inclure WHERE id = ..."
    
    # 5. DELETE FROM metadata WHERE id = ... (suppression de segments obsolÃ¨tes)
    if sql_upper.startswith("DELETE FROM METADATA"):
        return validate_delete_segment_sql(sql)
    
    return False, "OpÃ©ration non autorisÃ©e. Permis: UPDATE metadata SET pilier, INSERT/UPDATE/DELETE piliers"


def execute_pilier_sql(sql: str) -> Dict[str, Any]:
    """
    ExÃ©cute une opÃ©ration pilier sur la base.
    
    Args:
        sql: RequÃªte SQL (opÃ©rations piliers uniquement)
        
    Returns:
        dict avec:
            - status: "success" ou "error"
            - operation: type d'opÃ©ration effectuÃ©e
            - rows_affected: nombre de lignes affectÃ©es
            - error: message d'erreur si Ã©chec
    """
    # 1. Valider
    is_valid, error = validate_pilier_sql(sql)
    if not is_valid:
        return {
            "status": "error",
            "error": error,
            "sql": sql
        }
    
    # 2. DÃ©terminer le type d'opÃ©ration
    sql_upper = sql.upper().strip()
    if sql_upper.startswith("INSERT"):
        operation = "INSERT"
    elif sql_upper.startswith("UPDATE"):
        operation = "UPDATE"
    elif sql_upper.startswith("DELETE"):
        operation = "DELETE"
    else:
        operation = "UNKNOWN"
    
    # 3. ExÃ©cuter
    try:
        conn = sqlite3.connect(str(DB_PATH))
        cursor = conn.execute(sql)
        conn.commit()
        
        rows_affected = cursor.rowcount
        last_id = cursor.lastrowid if operation == "INSERT" else None
        
        conn.close()
        
        result = {
            "status": "success",
            "operation": operation,
            "rows_affected": rows_affected,
            "sql": sql
        }
        
        if last_id:
            result["inserted_id"] = last_id
            
        return result
        
    except sqlite3.Error as e:
        return {
            "status": "error",
            "error": f"Erreur SQLite: {str(e)}",
            "sql": sql
        }
    except Exception as e:
        return {
            "status": "error",
            "error": f"Erreur inattendue: {str(e)}",
            "sql": sql
        }


def get_piliers(categorie: str = None) -> Dict[str, Any]:
    """
    RÃ©cupÃ¨re les piliers de l'Agent.
    
    Args:
        categorie: Filtrer par catÃ©gorie (optionnel)
        
    Returns:
        dict avec status et results
    """
    sql = "SELECT * FROM piliers"
    if categorie:
        sql += f" WHERE categorie = '{categorie}'"
    sql += " ORDER BY importance DESC, updated_at DESC"
    
    try:
        conn = sqlite3.connect(str(DB_PATH))
        conn.row_factory = sqlite3.Row
        cursor = conn.execute(sql)
        rows = cursor.fetchall()
        results = [dict(row) for row in rows]
        conn.close()
        
        return {
            "status": "success",
            "results": results,
            "count": len(results)
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }

# === CONSULTATION DE SEGMENTS ===

def get_segments(
    limit: int = 10,
    order: str = "DESC",
    offset: int = 0,
    segment_id: int = None,
    fields: List[str] = None
) -> Dict[str, Any]:
    """
    RÃ©cupÃ¨re des segments de metadata pour consultation.
    
    Cas d'usage:
    - Voir les N segments les plus anciens/rÃ©cents
    - RÃ©cupÃ©rer un segment spÃ©cifique par ID (avant suppression)
    - Paginer Ã  travers les segments
    
    Args:
        limit: Nombre de segments Ã  retourner (dÃ©faut: 10, max: 50)
        order: "ASC" (plus anciens d'abord) ou "DESC" (plus rÃ©cents d'abord)
        offset: Pour pagination (dÃ©faut: 0)
        segment_id: Si fourni, retourne uniquement ce segment
        fields: Liste de champs Ã  retourner (dÃ©faut: champs les plus utiles)
        
    Returns:
        dict avec:
            - status: "success" ou "error"
            - results: liste de segments
            - count: nombre de rÃ©sultats retournÃ©s
            - total: nombre total de segments dans la base
    """
    try:
        conn = sqlite3.connect(str(DB_PATH))
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # Champs par dÃ©faut (les plus utiles pour consultation)
        # Note: basÃ© sur schÃ©ma metadata.db rÃ©el (pas de colonne 'domaine')
        default_fields = [
            "id", "timestamp", "source_file", "resume_texte",
            "type_contenu", "personnes", "projets", "auteur"
        ]
        selected_fields = fields if fields else default_fields
        fields_str = ", ".join(selected_fields)
        
        # Cas 1: Segment spÃ©cifique par ID
        if segment_id is not None:
            cursor.execute(f"SELECT {fields_str} FROM metadata WHERE id = ?", (segment_id,))
            row = cursor.fetchone()
            
            if row:
                results = [dict(row)]
                count = 1
            else:
                results = []
                count = 0
        
        # Cas 2: Liste paginÃ©e
        else:
            # Validation des paramÃ¨tres
            limit = min(max(1, limit), 50)  # Entre 1 et 50
            order = "ASC" if order.upper() == "ASC" else "DESC"
            offset = max(0, offset)
            
            cursor.execute(f"""
                SELECT {fields_str} FROM metadata 
                ORDER BY timestamp {order}
                LIMIT ? OFFSET ?
            """, (limit, offset))
            
            results = [dict(row) for row in cursor.fetchall()]
            count = len(results)
        
        # Compter le total de segments dans la base
        cursor.execute("SELECT COUNT(*) FROM metadata")
        total = cursor.fetchone()[0]
        
        conn.close()
        
        return {
            "status": "success",
            "results": results,
            "count": count,
            "total": total
        }
        
    except sqlite3.Error as e:
        return {
            "status": "error",
            "error": f"Erreur SQLite: {str(e)}"
        }
    except Exception as e:
        return {
            "status": "error",
            "error": f"Erreur inattendue: {str(e)}"
        }

# === SUPPRESSION DE SEGMENTS ===

def delete_segment(segment_id: int, reason: str = None) -> Dict[str, Any]:
    """
    Supprime un segment de metadata et retisse la toile ArachnÃ©.
    
    Workflow:
    1. VÃ©rifier que le segment existe
    2. Logger l'action (audit trail)
    3. Supprimer les liens orphelins dans edges
    4. Supprimer le segment de metadata
    5. Retisser la toile ArachnÃ©
    
    Args:
        segment_id: ID du segment Ã  supprimer
        reason: Raison de la suppression (optionnel, pour audit)
        
    Returns:
        dict avec:
            - status: "success" ou "error"
            - segment_id: ID du segment supprimÃ©
            - reason: raison fournie
            - edges_deleted: nombre de liens supprimÃ©s
            - arachne_status: rÃ©sultat du re-tissage
            - error: message d'erreur si Ã©chec
    """
    import logging
    from datetime import datetime
    
    try:
        conn = sqlite3.connect(str(DB_PATH))
        cursor = conn.cursor()
        
        # 1. VÃ©rifier que le segment existe
        cursor.execute("SELECT id, resume_texte FROM metadata WHERE id = ?", (segment_id,))
        row = cursor.fetchone()
        if not row:
            conn.close()
            return {
                "status": "error",
                "error": f"Segment {segment_id} introuvable",
                "segment_id": segment_id
            }
        
        resume_preview = row[1][:100] if row[1] else "N/A"
        
        # 2. Logger l'action (audit trail)
        timestamp = datetime.utcnow().isoformat()
        logging.info(f"[DELETE_SEGMENT] {timestamp} | ID: {segment_id} | Raison: {reason or 'Non spÃ©cifiÃ©e'} | AperÃ§u: {resume_preview}...")
        
        # 3. Supprimer les liens orphelins dans edges
        cursor.execute("""
            SELECT COUNT(*) FROM edges 
            WHERE source_id = ? OR target_id = ?
        """, (segment_id, segment_id))
        edges_count = cursor.fetchone()[0]
        
        cursor.execute("""
            DELETE FROM edges 
            WHERE source_id = ? OR target_id = ?
        """, (segment_id, segment_id))
        
        # 4. Supprimer le segment de metadata
        cursor.execute("DELETE FROM metadata WHERE id = ?", (segment_id,))
        
        conn.commit()
        conn.close()
        
        # 5. Retisser la toile ArachnÃ©
        arachne_result = retisser_toile()
        
        return {
            "status": "success",
            "segment_id": segment_id,
            "reason": reason,
            "resume_preview": resume_preview,
            "edges_deleted": edges_count,
            "arachne_status": arachne_result.get("status"),
            "arachne_liens": arachne_result.get("total_liens", 0),
            "timestamp": timestamp
        }
        
    except sqlite3.Error as e:
        return {
            "status": "error",
            "error": f"Erreur SQLite: {str(e)}",
            "segment_id": segment_id
        }
    except Exception as e:
        return {
            "status": "error",
            "error": f"Erreur inattendue: {str(e)}",
            "segment_id": segment_id
        }



def retisser_toile() -> Dict[str, Any]:
    """
    Relance ArachnÃ© pour reconstruire entiÃ¨rement la toile de liens.
    
    v2.2 - Ajout des tissages MEME_GROUPE et TAGS_PARTAGES
    
    AppelÃ© automatiquement aprÃ¨s delete_segment(), mais peut aussi
    Ãªtre appelÃ© manuellement pour maintenance.
    
    Returns:
        dict avec:
            - status: "success" ou "error"
            - total_liens: nombre total de liens aprÃ¨s tissage
            - details: breakdown par type de lien
    """
    import logging
    
    try:
        conn = sqlite3.connect(str(DB_PATH))
        cursor = conn.cursor()
        
        # Vider la table edges avant re-tissage
        cursor.execute("DELETE FROM edges")
        conn.commit()
        
        # Importer et exÃ©cuter ArachnÃ© v2.2
        try:
            from agents.arachne import (
                init_arachne_web, 
                tisser_entites, 
                tisser_emotions,
                tisser_groupes_thematiques,
                tisser_tags_partages
            )
        except ImportError:
            # Fallback si import direct Ã©choue
            import sys
            from pathlib import Path
            sys.path.insert(0, str(Path(__file__).parent.parent))
            from agents.arachne import (
                init_arachne_web, 
                tisser_entites, 
                tisser_emotions,
                tisser_groupes_thematiques,
                tisser_tags_partages
            )
        
        # Initialiser la structure
        init_arachne_web(conn)
        
        # === TISSAGE v2.1 (existant) ===
        nb_personnes = tisser_entites(conn, "personnes", "LIEN_PERSONNE")
        nb_projets = tisser_entites(conn, "projets", "LIEN_PROJET")
        nb_emotions = tisser_emotions(conn)
        
        # === TISSAGE v2.2 (nouveau) ===
        nb_groupes = tisser_groupes_thematiques(conn)
        nb_tags = tisser_tags_partages(conn)
        
        conn.close()
        
        total = nb_personnes + nb_projets + nb_emotions + nb_groupes + nb_tags
        
        logging.info(f"[ARACHNÃ‰ v2.2] Toile retissÃ©e: {total} liens")
        logging.info(f"   ðŸ‘¥ {nb_personnes} | ðŸš€ {nb_projets} | â¤ï¸ {nb_emotions} | ðŸ§© {nb_groupes} | ðŸ·ï¸ {nb_tags}")
        
        return {
            "status": "success",
            "total_liens": total,
            "details": {
                "LIEN_PERSONNE": nb_personnes,
                "LIEN_PROJET": nb_projets,
                "RESONANCE_EMOTION": nb_emotions,
                "MEME_GROUPE": nb_groupes,       # v2.2
                "TAGS_PARTAGES": nb_tags         # v2.2
            }
        }
        
    except Exception as e:
        logging.error(f"[ARACHNÃ‰] Erreur re-tissage: {str(e)}")
        return {
            "status": "error",
            "error": str(e)
        }


def validate_delete_segment_sql(sql: str) -> tuple[bool, str]:
    """
    Valide qu'une requÃªte DELETE sur metadata est autorisÃ©e.
    
    Seule forme permise: DELETE FROM metadata WHERE id = ...
    
    Returns:
        (is_valid, error_message)
    """
    sql_upper = sql.upper().strip()
    sql_clean = ' '.join(sql_upper.split())
    
    if not sql_upper.startswith("DELETE FROM METADATA"):
        return False, "Seul DELETE FROM metadata est autorisÃ©"
    
    if "WHERE" not in sql_upper:
        return False, "DELETE FROM metadata DOIT inclure une clause WHERE"
    
    if "ID" not in sql_upper:
        return False, "DELETE FROM metadata doit filtrer par ID (WHERE id = ...)"
    
    # Interdire les suppressions multiples dangereuses
    dangerous_patterns = ["WHERE 1", "WHERE TRUE", "WHERE ID >", "WHERE ID <", "WHERE ID !="]
    for pattern in dangerous_patterns:
        if pattern in sql_clean:
            return False, f"Pattern dangereux dÃ©tectÃ©: {pattern}"
    
    return True, ""

# === LIEN VERSION (MÃ©moire GÃ©nÃ©alogique) ===

def link_version(source_id: int, target_id: int) -> Dict[str, Any]:
    """
    CrÃ©e un lien LIEN_VERSION entre deux segments.
    Le source_id est l'ancien segment, target_id est le plus rÃ©cent qui le remplace/enrichit.
    
    Transforme la mÃ©moire cumulative en mÃ©moire gÃ©nÃ©alogique :
    - HermÃ¨s pourra filtrer automatiquement pour montrer le plus rÃ©cent
    - L'historique des versions reste accessible sur demande
    
    Args:
        source_id: ID du segment ancien (version antÃ©rieure)
        target_id: ID du segment plus rÃ©cent (version actuelle)
        
    Returns:
        dict avec:
            - status: "success" ou "error"
            - source_id, target_id: les IDs liÃ©s
            - type: "LIEN_VERSION"
            - message: confirmation lisible
            - error: message d'erreur si Ã©chec
    """
    import logging
    import json
    from datetime import datetime
    
    try:
        conn = sqlite3.connect(str(DB_PATH))
        cursor = conn.cursor()
        
        # 1. VÃ©rifier que les deux segments existent
        cursor.execute(
            "SELECT id, timestamp, resume_texte FROM metadata WHERE id IN (?, ?)", 
            (source_id, target_id)
        )
        rows = cursor.fetchall()
        
        if len(rows) != 2:
            found_ids = [r[0] for r in rows]
            missing = [sid for sid in [source_id, target_id] if sid not in found_ids]
            conn.close()
            return {
                "status": "error",
                "error": f"Segment(s) introuvable(s): {missing}",
                "source_id": source_id,
                "target_id": target_id
            }
        
        # 2. Organiser les donnÃ©es des segments
        segments = {
            r[0]: {
                "timestamp": r[1], 
                "resume": r[2][:100] if r[2] else "N/A"
            } 
            for r in rows
        }
        
        # 3. VÃ©rifier que source est bien plus ancien que target
        if segments[source_id]["timestamp"] > segments[target_id]["timestamp"]:
            conn.close()
            return {
                "status": "error",
                "error": f"source_id ({source_id}) doit Ãªtre plus ancien que target_id ({target_id})",
                "source_id": source_id,
                "target_id": target_id
            }
        
        # 4. CrÃ©er le lien LIEN_VERSION
        metadata = json.dumps({
            "created_at": datetime.utcnow().isoformat(),
            "source_resume": segments[source_id]["resume"],
            "target_resume": segments[target_id]["resume"]
        })
        
        cursor.execute("""
            INSERT OR REPLACE INTO edges (source_id, target_id, type, poids, metadata)
            VALUES (?, ?, 'LIEN_VERSION', 1.0, ?)
        """, (source_id, target_id, metadata))
        
        conn.commit()
        conn.close()
        
        logging.info(f"[LINK_VERSION] {source_id} â†’ {target_id}")
        
        return {
            "status": "success",
            "source_id": source_id,
            "target_id": target_id,
            "type": "LIEN_VERSION",
            "message": f"Version liÃ©e : {source_id} (ancien) â†’ {target_id} (actuel)"
        }
        
    except sqlite3.Error as e:
        return {
            "status": "error",
            "error": f"Erreur SQLite: {str(e)}",
            "source_id": source_id,
            "target_id": target_id
        }
    except Exception as e:
        return {
            "status": "error",
            "error": f"Erreur inattendue: {str(e)}",
            "source_id": source_id,
            "target_id": target_id
        }
# === AUTO-INJECTION IRIS (Jardin PrivÃ©) ===
# v2.3 - Migration iris_knowledge.db - 2026-01-26
# Les rÃ©flexions d'Iris sont maintenant stockÃ©es dans sa mÃ©moire sÃ©mantique

def write_reflection(
    contenu: str,
    type_reflexion: str = "intuition",
    poids_mnemique: float = 0.5,
    climat_session: str = None,
    tags_roget: List[str] = None,
    projets: List[str] = None,
    personnes: List[str] = None,
    ego_version: str = "Iris_2.1",
    modele: str = "gemini-3-flash-preview"
) -> Dict[str, Any]:
    """
    Permet Ã  Iris d'Ã©crire une rÃ©flexion dans sa mÃ©moire sÃ©mantique (iris_knowledge.db).
    
    C'est le mÃ©canisme de "rÃ©-entrÃ©e" (Edelman) : l'agent se traite 
    lui-mÃªme comme entrÃ©e sensorielle, crÃ©ant une boucle de conscience.
    
    Types de rÃ©flexion:
        - intuition    : Pressentiment, connexion non prouvÃ©e
        - brouillon    : Travail en cours, idÃ©e non finalisÃ©e
        - analyse      : Analyse structurÃ©e d'un problÃ¨me
        - etat_mental  : Ã‰tat cognitif actuel (continuitÃ© entre sessions)
        - fil_ariane   : Piste Ã  explorer, thread ouvert
        - heuristique  : Pattern dÃ©tectÃ© sur l'utilisateur ou le systÃ¨me
    
    Args:
        contenu: Le texte de la rÃ©flexion
        type_reflexion: Type de contenu (voir ci-dessus)
        poids_mnemique: Importance 0.0 (Ã©phÃ©mÃ¨re) â†’ 1.0 (inoubliable)
        climat_session: AtmosphÃ¨re de la session
        tags_roget: Tags sÃ©mantiques optionnels
        projets: Projets liÃ©s
        personnes: Personnes mentionnÃ©es
        ego_version: Version de l'ego au moment de l'Ã©criture
        modele: ModÃ¨le LLM utilisÃ© (le "corps", pas l'"Ã¢me")
        
    Returns:
        dict avec:
            - status: "success" ou "error"
            - knowledge_id: ID de l'entrÃ©e crÃ©Ã©e dans iris_knowledge.db
            - type: type de rÃ©flexion
            - message: confirmation lisible
    """
    import json
    import logging
    from datetime import datetime
    
    # Validation du type
    types_valides = ["intuition", "brouillon", "analyse", "etat_mental", "fil_ariane", "heuristique"]
    if type_reflexion not in types_valides:
        return {
            "status": "error",
            "error": f"Type invalide: {type_reflexion}. Valides: {types_valides}"
        }
    
    # Validation du poids â†’ importance (1-5)
    poids_mnemique = max(0.0, min(1.0, poids_mnemique))
    importance = max(1, min(5, int(poids_mnemique * 5) + 1))  # Convertir 0.0-1.0 â†’ 1-5
    
    try:
        conn = sqlite3.connect(str(IRIS_KNOWLEDGE_DB))
        cursor = conn.cursor()
        
        timestamp = datetime.utcnow().isoformat()
        
        # Construire le sujet (identifiant unique)
        date_str = timestamp[:10]
        sujet = f"{type_reflexion}_{date_str}_{timestamp[11:19].replace(':', '')}"
        
        # MÃ©tadonnÃ©es enrichies
        metadata = {
            "type_reflexion": type_reflexion,
            "poids_mnemique": poids_mnemique,
            "climat_session": climat_session,
            "tags_roget": tags_roget or [],
            "projets": projets or [],
            "personnes": personnes or [],
            "ego_version": ego_version,
            "modele": modele
        }
        
        # Domaine basÃ© sur le type
        domaine = f"reflexion_{type_reflexion}"
        
        # INSERT dans iris_knowledge.db
        cursor.execute("""
            INSERT INTO connaissances (
                domaine, sujet, information, importance, metadata,
                date_creation, derniere_maj
            ) VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (
            domaine,
            sujet,
            contenu,
            importance,
            json.dumps(metadata),
            timestamp,
            timestamp
        ))
        
        knowledge_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        logging.info(f"[IRIS_WRITE] {type_reflexion} #{knowledge_id} | Importance: {importance} | {contenu[:50]}...")
        
        return {
            "status": "success",
            "knowledge_id": knowledge_id,
            "type": type_reflexion,
            "importance": importance,
            "poids_mnemique": poids_mnemique,
            "ego_version": ego_version,
            "modele": modele,
            "timestamp": timestamp,
            "message": f"RÃ©flexion '{type_reflexion}' gravÃ©e (#{knowledge_id}, importance: {importance})"
        }
        
    except sqlite3.Error as e:
        return {
            "status": "error",
            "error": f"Erreur SQLite: {str(e)}"
        }
    except Exception as e:
        return {
            "status": "error",
            "error": f"Erreur inattendue: {str(e)}"
        }


def read_my_reflections(
    type_reflexion: str = None,
    limit: int = 10,
    poids_min: float = None,
    order: str = "DESC",
    ego_version: str = None,
    modele: str = None
) -> Dict[str, Any]:
    """
    Permet Ã  Iris de relire ses propres rÃ©flexions depuis iris_knowledge.db.
    
    C'est la deuxiÃ¨me partie de la boucle de rÃ©-entrÃ©e :
    Iris peut se souvenir de ce qu'elle a PENSÃ‰, pas seulement
    de ce que Serge lui a dit.
    
    Args:
        type_reflexion: Filtrer par type (intuition, brouillon, etc.)
        limit: Nombre de rÃ©flexions Ã  retourner (max 50)
        poids_min: Filtrer par importance minimum (0.0-1.0 â†’ converti en 1-5)
        order: "DESC" (plus rÃ©centes) ou "ASC" (plus anciennes)
        ego_version: Filtrer par version de l'ego (dans metadata JSON)
        modele: Filtrer par modÃ¨le LLM (dans metadata JSON)
        
    Returns:
        dict avec:
            - status: "success" ou "error"
            - results: liste des rÃ©flexions
            - count: nombre de rÃ©sultats
            - filters_applied: filtres utilisÃ©s
    """
    import json
    
    try:
        conn = sqlite3.connect(str(IRIS_KNOWLEDGE_DB))
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # Construire la requÃªte avec filtres
        conditions = ["domaine LIKE 'reflexion_%'"]
        params = []
        filters_applied = {"source": "iris_knowledge.db"}
        
        if type_reflexion:
            conditions.append("domaine = ?")
            params.append(f"reflexion_{type_reflexion}")
            filters_applied["type_reflexion"] = type_reflexion
        
        if poids_min is not None:
            # Convertir poids (0.0-1.0) en importance (1-5)
            importance_min = max(1, int(poids_min * 5) + 1)
            conditions.append("importance >= ?")
            params.append(importance_min)
            filters_applied["poids_min"] = poids_min
            filters_applied["importance_min"] = importance_min
        
        # Validation
        limit = min(max(1, limit), 50)
        order = "ASC" if order.upper() == "ASC" else "DESC"
        
        where_clause = " AND ".join(conditions)
        
        cursor.execute(f"""
            SELECT 
                id, domaine, sujet, information, importance, metadata,
                date_creation, derniere_maj
            FROM connaissances 
            WHERE {where_clause}
            ORDER BY date_creation {order}
            LIMIT ?
        """, params + [limit])
        
        rows = cursor.fetchall()
        
        # Transformer les rÃ©sultats
        results = []
        for row in rows:
            row_dict = dict(row)
            # Parser le metadata JSON
            try:
                meta = json.loads(row_dict.get("metadata", "{}"))
            except:
                meta = {}
            
            # Filtrage par ego_version ou modele (dans metadata)
            if ego_version and meta.get("ego_version") != ego_version:
                continue
            if modele and meta.get("modele") != modele:
                continue
            
            # Extraire le type depuis le domaine
            domaine = row_dict.get("domaine", "")
            type_from_domaine = domaine.replace("reflexion_", "") if domaine.startswith("reflexion_") else domaine
            
            results.append({
                "id": row_dict["id"],
                "timestamp": row_dict["date_creation"],
                "type_contenu": type_from_domaine,
                "resume_texte": row_dict["information"][:200] if row_dict["information"] else "",
                "information_complete": row_dict["information"],
                "poids_mnemique": meta.get("poids_mnemique", row_dict["importance"] / 5.0),
                "importance": row_dict["importance"],
                "climat_session": meta.get("climat_session"),
                "ego_version": meta.get("ego_version"),
                "modele": meta.get("modele"),
                "tags_roget": meta.get("tags_roget", []),
                "projets": meta.get("projets", []),
                "personnes": meta.get("personnes", [])
            })
        
        # Compter le total de rÃ©flexions
        cursor.execute(f"SELECT COUNT(*) FROM connaissances WHERE {where_clause}", params)
        total = cursor.fetchone()[0]
        
        conn.close()
        
        if ego_version:
            filters_applied["ego_version"] = ego_version
        if modele:
            filters_applied["modele"] = modele
        
        return {
            "status": "success",
            "results": results,
            "count": len(results),
            "total_reflexions": total,
            "filters_applied": filters_applied
        }
        
    except sqlite3.Error as e:
        return {
            "status": "error",
            "error": f"Erreur SQLite: {str(e)}"
        }
    except Exception as e:
        return {
            "status": "error",
            "error": f"Erreur inattendue: {str(e)}"
        }


def get_last_mental_state() -> Dict[str, Any]:
    """
    RÃ©cupÃ¨re le dernier Ã©tat mental d'Iris depuis iris_knowledge.db.
    
    C'est ce qu'Iris lit en premier au "rÃ©veil" pour savoir
    oÃ¹ elle en Ã©tait dans sa rÃ©flexion.
    
    Returns:
        dict avec:
            - status: "success" ou "error"
            - last_state: le dernier etat_mental ou None
            - days_since: nombre de jours depuis le dernier Ã©tat
    """
    import json
    from datetime import datetime
    
    try:
        conn = sqlite3.connect(str(IRIS_KNOWLEDGE_DB))
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT 
                id, domaine, sujet, information, importance, metadata,
                date_creation, derniere_maj
            FROM connaissances 
            WHERE domaine = 'reflexion_etat_mental'
            ORDER BY date_creation DESC
            LIMIT 1
        """)
        
        row = cursor.fetchone()
        conn.close()
        
        if row:
            row_dict = dict(row)
            try:
                meta = json.loads(row_dict.get("metadata", "{}"))
            except:
                meta = {}
            
            # Calculer le temps Ã©coulÃ©
            last_timestamp = datetime.fromisoformat(row_dict["date_creation"].replace("Z", "+00:00"))
            now = datetime.utcnow()
            days_since = (now - last_timestamp.replace(tzinfo=None)).days
            
            result = {
                "id": row_dict["id"],
                "timestamp": row_dict["date_creation"],
                "resume_texte": row_dict["information"][:500] if row_dict["information"] else "",
                "information_complete": row_dict["information"],
                "poids_mnemique": meta.get("poids_mnemique", row_dict["importance"] / 5.0),
                "climat_session": meta.get("climat_session"),
                "ego_version": meta.get("ego_version"),
                "modele": meta.get("modele")
            }
            
            return {
                "status": "success",
                "last_state": result,
                "days_since": days_since,
                "message": f"Dernier Ã©tat mental il y a {days_since} jour(s)"
            }
        else:
            return {
                "status": "success",
                "last_state": None,
                "days_since": None,
                "message": "Aucun Ã©tat mental enregistrÃ©. Premier rÃ©veil?"
            }
            
    except sqlite3.Error as e:
        return {
            "status": "error",
            "error": f"Erreur SQLite: {str(e)}"
        }
    except Exception as e:
        return {
            "status": "error",
            "error": f"Erreur inattendue: {str(e)}"
        }


import sqlite3
import json
from pathlib import Path
from typing import Dict, List, Any

DB_PATH = Path("~/Dropbox/aiterego_memory/metadata.db").expanduser()

# Types de liens disponibles (rÃ©fÃ©rence)
LINK_TYPES = {
    "LIEN_PERSONNE": {"poids": 1.5, "description": "Segments partageant une personne"},
    "LIEN_PROJET": {"poids": 1.5, "description": "Segments partageant un projet"},
    "RESONANCE_EMOTION": {"poids": 1.2, "description": "Segments avec Ã©motion similaire"},
    "MEME_GROUPE": {"poids": 1.8, "description": "Segments du mÃªme bloc thÃ©matique (gr_id)"},
    "TAGS_PARTAGES": {"poids": 1.3, "description": "Segments partageant le mÃªme tag Roget"},
    "LIEN_VERSION": {"poids": 2.0, "description": "Versions d'un mÃªme sujet"}
}


def explore_links(
    segment_id: int,
    link_types: List[str] = None,
    depth: int = 1,
    max_results: int = 10
) -> Dict[str, Any]:
    """
    Explore les liens du graphe ArachnÃ© Ã  partir d'un segment.
    
    Permet Ã  Iris de naviguer dans la mÃ©moire par connexions plutÃ´t que
    par recherche textuelle. RÃ©duit les appels search_memory de 5 Ã  2.
    
    Flux recommandÃ©:
        1. search_memory â†’ trouve UN segment pertinent
        2. explore_links â†’ suit les liens du graphe (SQL pur ~10ms)
    
    Args:
        segment_id: ID du segment de dÃ©part
        link_types: Liste des types de liens Ã  suivre (None = tous)
                    Valeurs: LIEN_PERSONNE, LIEN_PROJET, RESONANCE_EMOTION,
                             MEME_GROUPE, TAGS_PARTAGES, LIEN_VERSION
        depth: Profondeur de navigation (1 = voisins directs, 2 = voisins des voisins)
               Maximum: 2 (pour Ã©viter explosion)
        max_results: Nombre maximum de segments liÃ©s Ã  retourner
        
    Returns:
        dict avec:
            - status: "success" ou "error"
            - segment_id: ID du segment de dÃ©part
            - links_found: nombre de liens trouvÃ©s
            - results: liste des segments liÃ©s avec mÃ©tadonnÃ©es
            - link_types_used: types de liens explorÃ©s
            - depth_reached: profondeur effective atteinte
            - error: message d'erreur si Ã©chec
    """
    import logging
    
    # Validation des paramÃ¨tres
    depth = min(max(1, depth), 2)  # Clamp entre 1 et 2
    max_results = min(max(1, max_results), 50)  # Clamp entre 1 et 50
    
    # Validation des types de liens
    valid_types = list(LINK_TYPES.keys())
    if link_types:
        link_types = [t.upper() for t in link_types if t.upper() in valid_types]
        if not link_types:
            return {
                "status": "error",
                "error": f"Aucun type de lien valide. Types disponibles: {valid_types}",
                "segment_id": segment_id
            }
    else:
        link_types = valid_types  # Tous les types par dÃ©faut
    
    try:
        conn = sqlite3.connect(str(DB_PATH))
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # 1. VÃ©rifier que le segment de dÃ©part existe
        cursor.execute("""
            SELECT id, timestamp, resume_texte, personnes, projets 
            FROM metadata WHERE id = ?
        """, (segment_id,))
        source_row = cursor.fetchone()
        
        if not source_row:
            conn.close()
            return {
                "status": "error",
                "error": f"Segment {segment_id} introuvable",
                "segment_id": segment_id
            }
        
        source_info = {
            "id": source_row["id"],
            "timestamp": source_row["timestamp"],
            "resume_texte": source_row["resume_texte"][:100] if source_row["resume_texte"] else "N/A"
        }
        
        # 2. Construire la requÃªte pour les liens
        type_placeholders = ",".join(["?" for _ in link_types])
        
        # Depth 1: voisins directs
        visited = {segment_id}
        current_level = [segment_id]
        all_results = []
        
        for current_depth in range(1, depth + 1):
            if not current_level:
                break
                
            next_level = []
            
            for current_id in current_level:
                # Trouver les voisins de current_id
                cursor.execute(f"""
                    SELECT 
                        e.source_id,
                        e.target_id,
                        e.type AS link_type,
                        e.poids,
                        e.metadata AS link_metadata,
                        m.id AS linked_id,
                        m.timestamp,
                        m.resume_texte,
                        m.personnes,
                        m.projets,
                        m.emotion_valence,
                        m.emotion_activation,
                        m.tags_roget,
                        m.auteur
                    FROM edges e
                    JOIN metadata m ON (
                        m.id = CASE 
                            WHEN e.source_id = ? THEN e.target_id 
                            ELSE e.source_id 
                        END
                    )
                    WHERE (e.source_id = ? OR e.target_id = ?)
                      AND e.type IN ({type_placeholders})
                    ORDER BY e.poids DESC
                """, [current_id, current_id, current_id] + link_types)
                
                rows = cursor.fetchall()
                
                for row in rows:
                    linked_id = row["linked_id"]
                    
                    if linked_id in visited:
                        continue
                    
                    visited.add(linked_id)
                    
                    # Parser les mÃ©tadonnÃ©es du lien
                    link_meta = {}
                    if row["link_metadata"]:
                        try:
                            link_meta = json.loads(row["link_metadata"])
                        except:
                            pass
                    
                    result = {
                        "linked_segment_id": linked_id,
                        "link_type": row["link_type"],
                        "poids": row["poids"],
                        "link_metadata": link_meta,
                        "depth": current_depth,
                        "timestamp": row["timestamp"],
                        "resume_texte": row["resume_texte"][:150] if row["resume_texte"] else "N/A",
                        "personnes": row["personnes"],
                        "projets": row["projets"],
                        "auteur": row["auteur"]
                    }
                    
                    # Ajouter info Ã©motionnelle si RESONANCE_EMOTION
                    if row["link_type"] == "RESONANCE_EMOTION":
                        result["emotion"] = {
                            "valence": row["emotion_valence"],
                            "activation": row["emotion_activation"]
                        }
                    
                    all_results.append(result)
                    next_level.append(linked_id)
            
            current_level = next_level
        
        conn.close()
        
        # Trier par poids dÃ©croissant et limiter
        all_results.sort(key=lambda x: (-x["poids"], x["timestamp"]))
        final_results = all_results[:max_results]
        
        # Statistiques par type de lien
        type_counts = {}
        for r in final_results:
            t = r["link_type"]
            type_counts[t] = type_counts.get(t, 0) + 1
        
        return {
            "status": "success",
            "segment_id": segment_id,
            "source_info": source_info,
            "links_found": len(final_results),
            "total_explored": len(all_results),
            "results": final_results,
            "link_types_used": link_types,
            "link_types_found": type_counts,
            "depth_reached": depth,
            "max_results_applied": len(all_results) > max_results
        }
        
    except sqlite3.Error as e:
        return {
            "status": "error",
            "error": f"Erreur SQLite: {str(e)}",
            "segment_id": segment_id
        }
    except Exception as e:
        return {
            "status": "error",
            "error": f"Erreur inattendue: {str(e)}",
            "segment_id": segment_id
        }

# === TEST ===
if __name__ == "__main__":
    print("=" * 60)
    print("HERMÃˆS SIMPLE - Test de l'exÃ©cuteur SQL (v0.8.5)")
    print("=" * 60)
    
    # Test 1: RequÃªte valide SELECT
    print("\n1. Test requÃªte SELECT valide...")
    sql = "SELECT timestamp, resume_texte FROM metadata WHERE resume_texte LIKE '%PythonAnywhere%' ORDER BY timestamp ASC LIMIT 3"
    result = execute_sql(sql)
    print(f"   Status: {result['status']}, Count: {result.get('count', 0)}")
    
    # Test 2: RequÃªte invalide (INSERT dans metadata)
    print("\n2. Test INSERT interdit dans metadata...")
    sql = "INSERT INTO metadata (id) VALUES (999)"
    result = execute_sql(sql)
    print(f"   Status: {result['status']}, Error: {result.get('error', 'N/A')}")
    
    # Test 3: UPDATE pilier autorisÃ©
    print("\n3. Test UPDATE pilier (validation seulement)...")
    sql = "UPDATE metadata SET pilier = 1 WHERE id = 12345"
    is_valid, error = validate_pilier_sql(sql)
    print(f"   Valid: {is_valid}, Error: {error}")
    
    # Test 4: UPDATE autre champ interdit
    print("\n4. Test UPDATE autre champ (doit Ã©chouer)...")
    sql = "UPDATE metadata SET resume_texte = 'hack' WHERE id = 12345"
    is_valid, error = validate_pilier_sql(sql)
    print(f"   Valid: {is_valid}, Error: {error}")
    
    # Test 5: INSERT dans piliers autorisÃ©
    print("\n5. Test INSERT piliers (validation seulement)...")
    sql = "INSERT INTO piliers (fait, categorie, importance) VALUES ('Test', 'test', 1)"
    is_valid, error = validate_pilier_sql(sql)
    print(f"   Valid: {is_valid}, Error: {error}")
    
    # Test 6: DELETE piliers avec WHERE
    print("\n6. Test DELETE piliers avec WHERE...")
    sql = "DELETE FROM piliers WHERE id = 999"
    is_valid, error = validate_pilier_sql(sql)
    print(f"   Valid: {is_valid}, Error: {error}")
    
    # Test 7: DELETE piliers sans WHERE (doit Ã©chouer)
    print("\n7. Test DELETE piliers sans WHERE (doit Ã©chouer)...")
    sql = "DELETE FROM piliers"
    is_valid, error = validate_pilier_sql(sql)
    print(f"   Valid: {is_valid}, Error: {error}")
    
    # Test 8: get_segments - 5 plus anciens
    print("\n8. Test get_segments (5 plus anciens)...")
    result = get_segments(limit=5, order="ASC")
    print(f"   Status: {result['status']}, Count: {result.get('count', 0)}, Total: {result.get('total', 0)}")
    if result['status'] == 'success' and result['results']:
        print(f"   Premier segment ID: {result['results'][0].get('id')}")
    
    # Test 9: get_segments - segment spÃ©cifique
    print("\n9. Test get_segments (segment_id=1)...")
    result = get_segments(segment_id=1)
    print(f"   Status: {result['status']}, Count: {result.get('count', 0)}")
    
    print("\n" + "=" * 60)
    print("âœ… Tests terminÃ©s!")